{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a9257f",
   "metadata": {},
   "source": [
    "## MSBX 5420 Assignment 4\n",
    "This assignment includes two parts: (1) Graph analysis with Spark GraphFrames (Task 1 and 2); (2) Load data from employees data and do a simple analysis (Task 3). Three sets of data are used in the assignment: facebook social networks (Task 1), reddit community links (Task 2), and two data files related to employees in an organization (Task 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e9fd8",
   "metadata": {},
   "source": [
    "### Task 1 - Graph Analysis on Facebook Networks\n",
    "\n",
    "The data is from Facebook circles. For social networks, the data sometimes looks simple but boring - to protect privacy, only (recoded) user id is available and each row in the data is the connection or friendship from one user to another. \n",
    "\n",
    "Let's first load graphframes package and build the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9404fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[4]') \\\n",
    "                    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "                    .config(\"spark.jars\", \"graphframes-0.8.3-spark3.5-s_2.12.jar\") \\\n",
    "                    .config(\"spark.packages\", \"graphframes:graphframes:0.8.3-spark3.5-s_2.12\") \\\n",
    "                    .appName('spark_graph').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ade8279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure graphframes-0.8.3-spark3.5-s_2.12.jar is under same directory\n",
    "sc = spark.sparkContext\n",
    "sc.addPyFile('graphframes-0.8.3-spark3.5-s_2.12.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a8c9c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|from| to|\n",
      "+----+---+\n",
      "|   0|  1|\n",
      "|   0|  2|\n",
      "|   0|  3|\n",
      "|   0|  4|\n",
      "|   0|  5|\n",
      "|   0|  6|\n",
      "|   0|  7|\n",
      "|   0|  8|\n",
      "|   0|  9|\n",
      "|   0| 10|\n",
      "|   0| 11|\n",
      "|   0| 12|\n",
      "|   0| 13|\n",
      "|   0| 14|\n",
      "|   0| 15|\n",
      "|   0| 16|\n",
      "|   0| 17|\n",
      "|   0| 18|\n",
      "|   0| 19|\n",
      "|   0| 20|\n",
      "+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#first read the dataset\n",
    "import pyspark.sql.functions as fn\n",
    "\n",
    "#this is a txt file without header so after reading data we use .toDF() to add column names\n",
    "fb_connection = spark.read.csv('./facebook_combined.txt.gz', sep=' ').toDF('from', 'to')\n",
    "fb_connection.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb96d61e",
   "metadata": {},
   "source": [
    "Create vertices and edges dataframes, with `id` for vertices, and `src` / `dst` for edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119b0d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4039"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create vertices dataframe\n",
    "fb_vertices = fb_connection.select(fn.col('from').alias('id')).union(fb_connection.select(fn.col('to').alias('id'))).distinct()\n",
    "fb_vertices.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51348ad0",
   "metadata": {},
   "source": [
    "Because Graphframes by default uses multi-directed graph and there is no \"undirected\" definition, we need to \"duplicate\" the edges to have two edges between two nodes to capture their friend relationship on Facebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e19e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|src| dst|\n",
      "+---+----+\n",
      "|  0| 258|\n",
      "|  0| 294|\n",
      "|  9| 134|\n",
      "| 13| 332|\n",
      "| 40| 200|\n",
      "| 81| 286|\n",
      "| 87| 291|\n",
      "|107|1161|\n",
      "|107|1452|\n",
      "|107|1522|\n",
      "|107|1541|\n",
      "|107|1640|\n",
      "|107|1829|\n",
      "|141| 258|\n",
      "|180| 302|\n",
      "|186| 325|\n",
      "|198| 351|\n",
      "|251| 281|\n",
      "|257| 344|\n",
      "|348| 350|\n",
      "+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "176468"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create edges dataframe\n",
    "fb_edges = fb_connection.union(fb_connection.select(fn.col('to').alias('from'),fn.col('from').alias('to'))) \\\n",
    "                        .withColumnRenamed('from', 'src').withColumnRenamed('to', 'dst').distinct()\n",
    "fb_edges.show()\n",
    "fb_edges.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d68809",
   "metadata": {},
   "source": [
    "In total the data contains 4,039 users and 176,468 edges (bi-directional friendship), consistent with the data description. Then we can build the graph with the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15cb73ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphFrame(v:[id: string], e:[src: string, dst: string])\n"
     ]
    }
   ],
   "source": [
    "from graphframes import *\n",
    "#build graph\n",
    "fb_graph = GraphFrame(fb_vertices, fb_edges)\n",
    "print(fb_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a05c9f",
   "metadata": {},
   "source": [
    "Let's first get degree centrality. Because friendship tie in Facebook is essentially undirected (bi-directional in our data setup), inDegree and outDegree are actually same here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e037a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|  id|inDegree|\n",
      "+----+--------+\n",
      "| 107|    1045|\n",
      "|1684|     792|\n",
      "|1912|     755|\n",
      "|3437|     547|\n",
      "|   0|     347|\n",
      "|2543|     294|\n",
      "|2347|     291|\n",
      "|1888|     254|\n",
      "|1800|     245|\n",
      "|1663|     235|\n",
      "|2266|     234|\n",
      "|1352|     234|\n",
      "| 483|     231|\n",
      "| 348|     229|\n",
      "|1730|     226|\n",
      "|1985|     224|\n",
      "|1941|     223|\n",
      "|2233|     222|\n",
      "|2142|     221|\n",
      "|1431|     220|\n",
      "+----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+---------+\n",
      "|  id|outDegree|\n",
      "+----+---------+\n",
      "| 107|     1045|\n",
      "|1684|      792|\n",
      "|1912|      755|\n",
      "|3437|      547|\n",
      "|   0|      347|\n",
      "|2543|      294|\n",
      "|2347|      291|\n",
      "|1888|      254|\n",
      "|1800|      245|\n",
      "|1663|      235|\n",
      "|2266|      234|\n",
      "|1352|      234|\n",
      "| 483|      231|\n",
      "| 348|      229|\n",
      "|1730|      226|\n",
      "|1985|      224|\n",
      "|1941|      223|\n",
      "|2233|      222|\n",
      "|2142|      221|\n",
      "|1431|      220|\n",
      "+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#because this is an undirected graph (Facebook only has friendship, not following / followed), inDegree and outDegree are same here\n",
    "fb_graph.inDegrees.sort(fn.desc(\"inDegree\")).show()\n",
    "fb_graph.outDegrees.sort(fn.desc(\"outDegree\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8906c575",
   "metadata": {},
   "source": [
    "Now let's calculate pagerank to see who are the important ones in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ab43627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|  id|          pagerank|\n",
      "+----+------------------+\n",
      "|3437| 30.70878054527504|\n",
      "| 107|28.081776400814803|\n",
      "|1684|25.699684286540457|\n",
      "|   0|25.427815288933193|\n",
      "|1912|15.679403098894708|\n",
      "| 348| 9.493951584316306|\n",
      "| 686| 8.950484478259174|\n",
      "|3980| 8.743705001067275|\n",
      "| 414|  7.30408236329689|\n",
      "| 698| 5.317428593441851|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#[Your Code] to calculate pagerank on the graph and display nodes with top pageranks\n",
    "results = fb_graph.pageRank(resetProbability=0.16, maxIter=10)\n",
    "\n",
    "top_pageranks = results.vertices.orderBy(results.vertices.pagerank.desc()).limit(10)\n",
    "top_pageranks.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d013a7",
   "metadata": {},
   "source": [
    "Shortest path is useful in many cases. Note that the `shortestPaths()` function in Grapgframes will actually calculate shortest distances (number of edges) from each node in the graph to all the nodes specified in `landmarks`. Here we want to calculate all the shortest paths from all users to two sample users with `id` of `0` and `25`, and then see the distribution of shortest distances from all users to them. So we first need to calculate shortest paths on the graph and extract the distance information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80b9d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Your Code] to calculate shortest paths from all nodes to node id 0 and 25\n",
    "shortest_path = fb_graph.shortestPaths(landmarks=['0','25'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e80ac6",
   "metadata": {},
   "source": [
    "Then we check the distribution of distances from all nodes to node 0 and 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b88a064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|distance|count|\n",
      "+--------+-----+\n",
      "|       0|    1|\n",
      "|       1|  347|\n",
      "|       2| 1171|\n",
      "|       3| 1742|\n",
      "|       4|  519|\n",
      "|       5|  117|\n",
      "|       6|  142|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+-----+\n",
      "|distance|count|\n",
      "+--------+-----+\n",
      "|       0|    1|\n",
      "|       1|   69|\n",
      "|       2|  278|\n",
      "|       3| 1171|\n",
      "|       4| 1742|\n",
      "|       5|  519|\n",
      "|       6|  117|\n",
      "|       7|  142|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check the distribution of distances to node 0 and 25\n",
    "shortest_path.select(fn.map_values('distances')[1].alias('distance')).groupBy('distance').count().orderBy('distance').show()\n",
    "shortest_path.select(fn.map_values('distances')[0].alias('distance')).groupBy('distance').count().orderBy('distance').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e57c6ce",
   "metadata": {},
   "source": [
    "Next we want to know the structure of this network, so we can get the clusters. We use label propagation to identify clusters, and show the number of clusters as well as size of clusters in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4a15579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|  id|label|\n",
      "+----+-----+\n",
      "|1788|   53|\n",
      "|1111|   53|\n",
      "|1045|   53|\n",
      "|1254|   53|\n",
      "|1368|   53|\n",
      "| 955|   53|\n",
      "|1030|   53|\n",
      "|1197|   53|\n",
      "|1252|   53|\n",
      "|1043|   53|\n",
      "| 909|   53|\n",
      "|1585|   59|\n",
      "|1055|   59|\n",
      "|1564|   59|\n",
      "|1050|   59|\n",
      "|1544|   59|\n",
      "|1822|   59|\n",
      "|1266|   59|\n",
      "|1441|   59|\n",
      "|1434|   59|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#[Your Code] to use label propagation to identify clusters in the network; then show the total number of clusters you get and the size of each cluster.\n",
    "clusters = fb_graph.labelPropagation(maxIter=5)\n",
    "clusters.orderBy(fn.asc('label')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ce11ea6-cd00-4f10-8b46-5d86a8982396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.select('label').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a707a39a-ae6b-4668-a118-0d2e39ab0a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  558|    6|\n",
      "|  442|   73|\n",
      "| 2371|    8|\n",
      "| 1447|    5|\n",
      "| 2317|   20|\n",
      "| 3125|  110|\n",
      "| 1979|   15|\n",
      "| 2775|    8|\n",
      "| 2975|    1|\n",
      "|  828|   17|\n",
      "| 2021|    6|\n",
      "| 2470|  156|\n",
      "| 1073|    7|\n",
      "| 2012|   28|\n",
      "| 1501|   19|\n",
      "| 2999|    5|\n",
      "| 3295|   21|\n",
      "| 1016|    7|\n",
      "| 2715|    5|\n",
      "| 1833|   73|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clusters.select('label').groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f08646",
   "metadata": {},
   "source": [
    "### Task 2 - Graph Analysis on Reddit Communities\n",
    "We will work on a different graph dataset from Reddit in Task 2. Reddit is a large community for discussing different topics. In reddit, there are subreddits for specific topics. In particular, one community (subreddit) links to another community (subreddit) when a post refers to another post in another community. Therefore, the data here contains the posts from 2014 to 2017 that contain hyperlinks of another different subreddit. The data contains two parts, one is the hyperlinks in the body of reddit posts, the other is the hyperlinks in the title of reddit posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60d40ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-------+----------------+--------------+\n",
      "|SOURCE_SUBREDDIT|TARGET_SUBREDDIT|POST_ID|       TIMESTAMP|LINK_SENTIMENT|\n",
      "+----------------+----------------+-------+----------------+--------------+\n",
      "| leagueoflegends| teamredditteams|1u4nrps|12/31/2013 16:39|             1|\n",
      "|      theredlion|          soccer| 1u4qkd|12/31/2013 18:18|            -1|\n",
      "|    inlandempire|          bikela|1u4qlzs|  1/1/2014 14:54|             1|\n",
      "|             nfl|             cfb|1u4sjvs|12/31/2013 17:37|             1|\n",
      "|      playmygame|         gamedev|1u4w5ss|   1/1/2014 2:51|             1|\n",
      "|      dogemarket|        dogecoin|1u4w7bs|12/31/2013 18:35|             1|\n",
      "|     locationbot|     legaladvice|1u4wfes|  1/7/2014 20:17|             1|\n",
      "|       indiefied|             aww|1u50pos|  3/3/2014 17:00|             1|\n",
      "|    posthardcore|      bestof2013|1u5ccus|12/31/2013 23:16|             1|\n",
      "|    posthardcore|        corejerk|1u5ccus|12/31/2013 23:16|             1|\n",
      "|          gfycat|           india|1u5df2s|12/31/2013 22:27|             1|\n",
      "|       metalcore|      bestof2013|1u5iets|   1/1/2014 4:15|             1|\n",
      "|       metalcore|        corejerk|1u5iets|   1/1/2014 4:15|             1|\n",
      "|    suicidewatch|      offmychest|1u5k33s|   1/1/2014 3:01|             1|\n",
      "|        dogecoin|        novacoin|1u5olgs|   1/1/2014 5:58|             1|\n",
      "|   gaming4gamers|         fallout|1u5q84s|   1/1/2014 6:55|             1|\n",
      "|            kpop|           dota2|1u5qg2s|   1/1/2014 7:05|             1|\n",
      "|         airsoft|   airsoftmarket|1u5r7js|   1/1/2014 7:09|             1|\n",
      "|     circlebroke|       childfree|1u5rs9s|   1/1/2014 6:51|             1|\n",
      "|          tribes|           games|1u5syks|   1/1/2014 9:06|             1|\n",
      "+----------------+----------------+-------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------+----------------+-------+----------------+--------------+\n",
      "| SOURCE_SUBREDDIT|TARGET_SUBREDDIT|POST_ID|       TIMESTAMP|LINK_SENTIMENT|\n",
      "+-----------------+----------------+-------+----------------+--------------+\n",
      "|       rddtgaming|        rddtrust|1u4pzzs|12/31/2013 16:39|             1|\n",
      "|          xboxone|   battlefield_4|1u4tmfs|12/31/2013 17:59|             1|\n",
      "|              ps4|   battlefield_4|1u4tmos|12/31/2013 17:59|             1|\n",
      "|fitnesscirclejerk|       leangains|1u50xfs|12/31/2013 19:01|             1|\n",
      "|fitnesscirclejerk|     lifeprotips|1u51nps|12/31/2013 21:02|             1|\n",
      "|           cancer|      fuckcancer|1u5216s|12/31/2013 18:59|             1|\n",
      "|          jleague|          soccer|1u53hzs|12/31/2013 18:29|             1|\n",
      "|       bestoftldr|            tifu|1u53lxs|  1/1/2014 18:47|             1|\n",
      "| quityourbullshit|            pics|1u55wws|12/31/2013 22:15|             1|\n",
      "|           bestof|      confession|1u56b3s|12/31/2013 21:41|             1|\n",
      "|     anarchychess|           funny|1u56bws|   1/6/2014 7:42|             1|\n",
      "|     internet_box|             ama|1u56qas|   1/1/2014 4:33|             1|\n",
      "|fitnesscirclejerk|           nofap|1u570zs|12/31/2013 21:02|             1|\n",
      "|            ffxiv|        ffxivapp|1u58d7s|12/31/2013 22:23|             1|\n",
      "|       switcharoo|           funny|1u58e6s|12/31/2013 19:35|             1|\n",
      "|    bitcoinmining|         bitcoin|1u58kus|12/31/2013 23:17|             1|\n",
      "|   subredditdrama|             nfl| 1u58yq|12/31/2013 21:59|            -1|\n",
      "|            corgi|        colorado|1u596as|   1/1/2014 0:58|             1|\n",
      "|       propaganda|      conspiracy|1u5acws|  1/1/2014 11:20|             1|\n",
      "|          orlando|           epcot|1u5ag6s|12/31/2013 20:10|             1|\n",
      "+-----------------+----------------+-------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "\n",
    "#read data, two data files in total\n",
    "reddit_link = spark.read.csv('./reddit_hyperlinks.csv.gz', header=True, inferSchema=True, sep=',')\n",
    "reddit_link.show()\n",
    "reddit_link_title = spark.read.csv('./reddit_hyperlinks_title.csv.gz', header=True, inferSchema=True, sep=',')\n",
    "reddit_link_title.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772986a0",
   "metadata": {},
   "source": [
    "Here we union the two dataframes first and then create the vertices/edges dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "169a6134",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_link_all = reddit_link.union(reddit_link_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e7e4f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67180"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create vertices dataframe\n",
    "reddit_vertices = reddit_link_all.select(fn.col('SOURCE_SUBREDDIT').alias('id')) \\\n",
    "                                 .union(reddit_link_all.select(fn.col('TARGET_SUBREDDIT').alias('id'))).distinct()\n",
    "reddit_vertices.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfecaff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+-------+----------------+--------------+\n",
      "|            src|            dst|POST_ID|       TIMESTAMP|LINK_SENTIMENT|\n",
      "+---------------+---------------+-------+----------------+--------------+\n",
      "|leagueoflegends|teamredditteams|1u4nrps|12/31/2013 16:39|             1|\n",
      "|     theredlion|         soccer| 1u4qkd|12/31/2013 18:18|            -1|\n",
      "|   inlandempire|         bikela|1u4qlzs|  1/1/2014 14:54|             1|\n",
      "|            nfl|            cfb|1u4sjvs|12/31/2013 17:37|             1|\n",
      "|     playmygame|        gamedev|1u4w5ss|   1/1/2014 2:51|             1|\n",
      "|     dogemarket|       dogecoin|1u4w7bs|12/31/2013 18:35|             1|\n",
      "|    locationbot|    legaladvice|1u4wfes|  1/7/2014 20:17|             1|\n",
      "|      indiefied|            aww|1u50pos|  3/3/2014 17:00|             1|\n",
      "|   posthardcore|     bestof2013|1u5ccus|12/31/2013 23:16|             1|\n",
      "|   posthardcore|       corejerk|1u5ccus|12/31/2013 23:16|             1|\n",
      "|         gfycat|          india|1u5df2s|12/31/2013 22:27|             1|\n",
      "|      metalcore|     bestof2013|1u5iets|   1/1/2014 4:15|             1|\n",
      "|      metalcore|       corejerk|1u5iets|   1/1/2014 4:15|             1|\n",
      "|   suicidewatch|     offmychest|1u5k33s|   1/1/2014 3:01|             1|\n",
      "|       dogecoin|       novacoin|1u5olgs|   1/1/2014 5:58|             1|\n",
      "|  gaming4gamers|        fallout|1u5q84s|   1/1/2014 6:55|             1|\n",
      "|           kpop|          dota2|1u5qg2s|   1/1/2014 7:05|             1|\n",
      "|        airsoft|  airsoftmarket|1u5r7js|   1/1/2014 7:09|             1|\n",
      "|    circlebroke|      childfree|1u5rs9s|   1/1/2014 6:51|             1|\n",
      "|         tribes|          games|1u5syks|   1/1/2014 9:06|             1|\n",
      "+---------------+---------------+-------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "858488"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create edges dataframe\n",
    "reddit_edges = reddit_link_all.withColumnRenamed('SOURCE_SUBREDDIT', 'src').withColumnRenamed('TARGET_SUBREDDIT', 'dst')\n",
    "reddit_edges.show()\n",
    "reddit_edges.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e7080",
   "metadata": {},
   "source": [
    "Now build the graph with the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "170e31ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphFrame(v:[id: string], e:[src: string, dst: string ... 3 more fields])\n"
     ]
    }
   ],
   "source": [
    "from graphframes import *\n",
    "#build graph\n",
    "reddit_graph = GraphFrame(reddit_vertices, reddit_edges)\n",
    "print(reddit_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc439fa",
   "metadata": {},
   "source": [
    "Let's start with degree centrality again. Here the importance of a community is better approximated by the links *to* the community (the posts in the community were referred in other communities), so we use inDegree centrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4eb234b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|             id|inDegree|\n",
      "+---------------+--------+\n",
      "|      askreddit|   26622|\n",
      "|           iama|   13446|\n",
      "|           pics|   12578|\n",
      "|  todayilearned|   11124|\n",
      "|          funny|   10777|\n",
      "|         videos|   10013|\n",
      "|      worldnews|    9944|\n",
      "|           news|    7692|\n",
      "|       politics|    6114|\n",
      "|         gaming|    6097|\n",
      "|  adviceanimals|    5503|\n",
      "|            wtf|    5320|\n",
      "|           gifs|    5214|\n",
      "| writingprompts|    5056|\n",
      "|leagueoflegends|    4856|\n",
      "|        science|    4557|\n",
      "|     the_donald|    4487|\n",
      "| showerthoughts|    4202|\n",
      "|        bitcoin|    4028|\n",
      "|            nfl|    4000|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reddit_graph.inDegrees.sort(fn.desc(\"inDegree\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f06af9",
   "metadata": {},
   "source": [
    "Now let's use pagerank to determine the importance of community and show the top ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fef25276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|           id|          pagerank|\n",
      "+-------------+------------------+\n",
      "|    askreddit|1458.0204619976894|\n",
      "|         iama|1193.7232770052267|\n",
      "|         pics| 748.4373997070086|\n",
      "|        funny| 664.6783281635264|\n",
      "|       videos| 619.5871312420129|\n",
      "|todayilearned| 464.0774208095912|\n",
      "|    worldnews| 418.1963576828606|\n",
      "|       gaming|390.47308004670685|\n",
      "|         news|  325.681508598367|\n",
      "|      science| 295.4638461772499|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#[Your Code] to use pagerank to identify the most important communities based on the hyperlinks and display the top ones\n",
    "results = reddit_graph.pageRank(resetProbability=0.16, maxIter=10)\n",
    "\n",
    "top_pageranks = results.vertices.orderBy(results.vertices.pagerank.desc()).limit(10)\n",
    "top_pageranks.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3827e8",
   "metadata": {},
   "source": [
    "In the data, one column is the sentiment of the post with hyperlinks from one subreddit to another. So we can learn whether or not this is a positive post referring another subreddit. In other words, some posts might be negative when referring to the posts in other subreddits, implying that some communities may have conflicts. Can you identify which pairs of communities are more likely to have conflicts?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f78305",
   "metadata": {},
   "source": [
    "To do this, we can perform a query on the edges in the graph. Basically, we can obtain the average sentiment (`LINK_SENTIMENT` column) from one subreddit to another. To make sure this is not random, we should ONLY consider those pairs of communities with *at least 10 hyperlinks from one to another*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5795ea86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------------------+\n",
      "|                 src|                 dst|link_count|   average_sentiment|\n",
      "+--------------------+--------------------+----------+--------------------+\n",
      "|         hearthstone|hearthstonecircle...|        39| -0.6410256410256411|\n",
      "|      subredditdrama|             offbeat|        51| -0.0196078431372549|\n",
      "|               drama|             worstof|        20|                -0.1|\n",
      "|      subredditdrama|         liverpoolfc|        12|-0.16666666666666666|\n",
      "|      subredditdrama|                dayz|        30|-0.06666666666666667|\n",
      "|        circlebroke2|                 nfl|        27| -0.1111111111111111|\n",
      "|               drama|       adviceanimals|        25|               -0.04|\n",
      "|         circlebroke|          cringepics|        17|-0.05882352941176...|\n",
      "|      subredditdrama| kitchenconfidential|        25|               -0.04|\n",
      "|     trueredditdrama|      subredditdrama|        24|-0.08333333333333333|\n",
      "|        circlebroke2|                 mma|        11| -0.6363636363636364|\n",
      "|        circlebroke2|              askmen|        18| -0.1111111111111111|\n",
      "|        circlebroke2|       unitedkingdom|        29|-0.03448275862068...|\n",
      "|               drama|        metanarchism|        22|-0.36363636363636365|\n",
      "|      subredditdrama|        streetfights|        21|-0.04761904761904...|\n",
      "|againsthatesubred...|     subredditcancer|        16|               -0.25|\n",
      "|           openbroke|           askreddit|        31|-0.03225806451612903|\n",
      "|              straya|           australia|        13|-0.07692307692307693|\n",
      "|   againstmensrights|      kotakuinaction|        17|-0.05882352941176...|\n",
      "|           gifbattle|                gifs|        10|                -0.2|\n",
      "+--------------------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#[Your Code] to identify the communities with significant conflicts\n",
    "edges_filtered = reddit_graph.edges.groupby(\"src\", \"dst\").agg(\n",
    "    fn.count(\"LINK_SENTIMENT\").alias(\"link_count\"),\n",
    "    fn.avg(\"LINK_SENTIMENT\").alias(\"average_sentiment\")\n",
    ").filter(\"link_count >= 10\")\n",
    "\n",
    "# Step 2: (The average sentiment is already calculated in the step above)\n",
    "\n",
    "# Step 3: Filter for negative sentiment (adjust the threshold as necessary)\n",
    "conflict_pairs = edges_filtered.filter(\"average_sentiment < 0\")\n",
    "\n",
    "# Show the results\n",
    "conflict_pairs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537f4c9",
   "metadata": {},
   "source": [
    "Next let's perform some searches on the graph. Assume you are a random walker in reddit communities - you just randomly browse posts without targeting any particular communities. Now assume you start your browsing trip in the `leagueoflegends` commuity (League of Legends is a Multiplayer Online Battle Arena (MOBA) e-sports video game). Now we are wondering whether (and in what way) you have a chance to reach other communities through the hyperlinks between communities. To do this, we can use breath-first search or motif finding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be040f",
   "metadata": {},
   "source": [
    "Note that this is not likely to be a real action in practice and it is also not the actual role of those hyperlinks. We just use it as a simulated case of graph search. Now let's first see if you can reach `politics` community from `leagueoflegends` community directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5274c46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|id |\n",
      "+---+\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paths = reddit_graph.bfs(fromExpr = \"id = 'leagueoflegends'\", toExpr = \"id = 'politics'\", maxPathLength = 1)\n",
    "paths.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d995eb4",
   "metadata": {},
   "source": [
    "It seems no direct hyperlinks from `leagueoflegends` subreddit to `politics` subreddit. Therefore, we should check if there are shortest paths with length of 2 so that we may still reach `politics` community through another community. Can you identify those paths through `both` breath-first search and motif finding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3112effe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------------------------------------------+--------+----------------------------------------------+----------+\n",
      "|from             |e0                                                    |v1      |e1                                            |to        |\n",
      "+-----------------+------------------------------------------------------+--------+----------------------------------------------+----------+\n",
      "|{leagueoflegends}|{leagueoflegends, greece, 47lojus, 2/25/2016 13:43, 1}|{greece}|{greece, politics, 56m7ofs, 10/9/2016 6:17, 1}|{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 5ujbqxs, 2/16/2017 17:18, 1}  |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 4xsgn5s, 8/15/2016 0:18, 1}   |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 3cpug8s, 7/9/2015 14:29, 1}   |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 3358v8, 4/19/2015 11:55, -1}  |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 2fam27s, 9/2/2014 15:04, 1}   |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 24sewns, 5/5/2014 11:13, 1}   |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 1zj9pts, 3/4/2014 7:31, 1}    |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 3gs0lws, 8/12/2015 16:18, 1}  |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 37am3f, 5/25/2015 23:21, -1}  |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 1ueax1s, 1/4/2014 8:32, 1}    |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 5ujbqxs, 2/16/2017 17:18, 1}  |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 4xsgn5s, 8/15/2016 0:18, 1}   |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 3cpug8s, 7/9/2015 14:29, 1}   |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 3358v8, 4/19/2015 11:55, -1}  |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 2fam27s, 9/2/2014 15:04, 1}   |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 24sewns, 5/5/2014 11:13, 1}   |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 1zj9pts, 3/4/2014 7:31, 1}    |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 3gs0lws, 8/12/2015 16:18, 1}  |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 37am3f, 5/25/2015 23:21, -1}  |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "+-----------------+------------------------------------------------------+--------+----------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#[Your Code] to use breath-first search to find possible shortest paths from leagueoflegends to politics\n",
    "paths_bfs = reddit_graph.bfs(\n",
    "    fromExpr=\"id = 'leagueoflegends'\", \n",
    "    toExpr=\"id = 'politics'\", \n",
    "    maxPathLength=2\n",
    ")\n",
    "paths_bfs.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73d0e5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------------------------------------------+--------+----------------------------------------------+----------+\n",
      "|a                |ab                                                    |b       |bc                                            |c         |\n",
      "+-----------------+------------------------------------------------------+--------+----------------------------------------------+----------+\n",
      "|{leagueoflegends}|{leagueoflegends, greece, 47lojus, 2/25/2016 13:43, 1}|{greece}|{greece, politics, 56m7ofs, 10/9/2016 6:17, 1}|{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 5ujbqxs, 2/16/2017 17:18, 1}  |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 4xsgn5s, 8/15/2016 0:18, 1}   |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 3cpug8s, 7/9/2015 14:29, 1}   |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 3358v8, 4/19/2015 11:55, -1}  |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 2fam27s, 9/2/2014 15:04, 1}   |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 24sewns, 5/5/2014 11:13, 1}   |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 1zj9pts, 3/4/2014 7:31, 1}    |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 3gs0lws, 8/12/2015 16:18, 1}  |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 37am3f, 5/25/2015 23:21, -1}  |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 1ueax1s, 1/4/2014 8:32, 1}    |{iama}  |{iama, politics, 1uideg, 1/5/2014 19:15, -1}  |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 5ujbqxs, 2/16/2017 17:18, 1}  |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 4xsgn5s, 8/15/2016 0:18, 1}   |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 3cpug8s, 7/9/2015 14:29, 1}   |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 3358v8, 4/19/2015 11:55, -1}  |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 2fam27s, 9/2/2014 15:04, 1}   |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 24sewns, 5/5/2014 11:13, 1}   |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 1zj9pts, 3/4/2014 7:31, 1}    |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 3gs0lws, 8/12/2015 16:18, 1}  |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "|{leagueoflegends}|{leagueoflegends, iama, 37am3f, 5/25/2015 23:21, -1}  |{iama}  |{iama, politics, 28p1h4s, 6/20/2014 20:29, 1} |{politics}|\n",
      "+-----------------+------------------------------------------------------+--------+----------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#[Your Code] to use motif finding to find possible shortest paths from leagueoflegends to politics\n",
    "motifs = reddit_graph.find(\"(a)-[ab]->(b); (b)-[bc]->(c)\")\n",
    "motifs = motifs.filter(\n",
    "    \"a.id = 'leagueoflegends' AND c.id = 'politics'\"\n",
    ")\n",
    "motifs.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bd2db6",
   "metadata": {},
   "source": [
    "### Task 3 - Analyze Organizational Network\n",
    "As the last task in all assignments, you will see no existing code and you will do a simple task on your own. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b09ab88",
   "metadata": {},
   "source": [
    "The task is to read data from employees data (part of the data from official MySQL database samples). You need to read data from the `dept_emp.csv` file (employee department) and `titles.csv` file (employee titles)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4901c",
   "metadata": {},
   "source": [
    "In the data, there are columns `from_date` and `to_date`. For `to_date`, if it is `9999-01-01`, the employee is still at the company by the time of data collection (current employee). Therefore, we want to filter those employees with `to_date` as `9999-01-01` and with more than one records in the `dept_emp` table. That's what you need to obtain - you can use either dataframe operations or sql, and use `.show()` to display the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12a5e4b-ac9a-4a32-a8da-edf7c1e86d15",
   "metadata": {},
   "source": [
    "After that, we load the `titles.csv` file as a dataframe. In the dataframe, we can again use `to_date` column to filter all employees' current titles. Then with `emp_no`, you can join two dataframes and then obtain the distribution of titles for current employees who have worked at more than one department. Still, you can use either dataframe operations or sql, and use `.show()` to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80226a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Your Code] to complete task 3\n",
    "dept_df = spark.read.csv('./dept_emp.csv', header=True, inferSchema=True, sep=',')\n",
    "titles_df = spark.read.csv('./titles.csv', header=True, inferSchema=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f954570b-af5b-4aee-8fca-11ebec098a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|emp_no|count|\n",
      "+------+-----+\n",
      "| 10206|    1|\n",
      "| 10623|    1|\n",
      "| 10817|    1|\n",
      "| 11033|    1|\n",
      "| 11141|    1|\n",
      "| 11317|    1|\n",
      "| 11458|    1|\n",
      "| 11748|    1|\n",
      "| 11858|    1|\n",
      "| 12027|    1|\n",
      "| 12799|    1|\n",
      "| 12940|    1|\n",
      "| 13289|    1|\n",
      "| 13623|    1|\n",
      "| 14570|    1|\n",
      "| 14832|    1|\n",
      "| 15447|    1|\n",
      "| 15619|    1|\n",
      "| 15727|    1|\n",
      "| 15790|    1|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_dept = dept_df.filter((fn.col('to_date') == '9999-01-01')) \\\n",
    "                        .groupBy('emp_no') \\\n",
    "                        .count() \\\n",
    "                        .filter(fn.col('count') >= 1)\n",
    "filtered_dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92f95688-4691-4d26-b6a0-a263a58f7d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_titles = titles_df.filter(fn.col('to_date') == '9999-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "686c70ae-7570-45fd-a892-e6e9502aeaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|             title|count|\n",
      "+------------------+-----+\n",
      "|   Senior Engineer|85939|\n",
      "|      Senior Staff|82024|\n",
      "|          Engineer|30983|\n",
      "|             Staff|25526|\n",
      "|  Technique Leader|12055|\n",
      "|Assistant Engineer| 3588|\n",
      "|           Manager|    9|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_df = filtered_dept.join(filtered_titles,'emp_no')\n",
    "\n",
    "title_distribution = full_df.groupBy('title').count().orderBy('count',ascending=False)\n",
    "title_distribution.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
