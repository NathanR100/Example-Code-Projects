{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upsdTirof_IA"
      },
      "source": [
        "# MSBC5180 HW5: Ensemble Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1co9m0NFf_IF"
      },
      "source": [
        "In this assignment, you will continue working with the Twitter sentiment dataset from HW4. This time, you will build a classifier that combines the individual classifiers submitted by everyone in the class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L31AV8Idf_IH"
      },
      "source": [
        "## Combined Dataset\n",
        "\n",
        "\n",
        "The probabilities from some of the submissions from HW4 have been put together for this assignment. The format is a CSV file where the first column is the label, and subsequent columns are classifier probabilities. Each three-column sequence is the probability of negative ($-1$), neutral ($0$), and positive ($1$), in that order. For example, column 2 (where column 1 is the label) is the negative probability, column 3 is the neural probability, and cololum column 4 is the positive probability from the first submission. Column 5 is the negative probability of the second submission, column 6 is the neutral probability of the second submission, and so on. There are two files: the first should be used for training and cross-validation, and the second should be used for testing.\n",
        "\n",
        "As usual, run the code below to load the data. The accuracies of each individual system are also calculated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGLvs23vf_IH",
        "outputId": "c5860fd6-111c-4b8d-b473-3fcaa4d21b29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission 1:\n",
            " Validation accuracy: 0.651113\n",
            " Test accuracy: 0.633333\n",
            "Submission 2:\n",
            " Validation accuracy: 0.616119\n",
            " Test accuracy: 0.600000\n",
            "Submission 3:\n",
            " Validation accuracy: 0.716861\n",
            " Test accuracy: 0.755556\n",
            "Submission 4:\n",
            " Validation accuracy: 0.752916\n",
            " Test accuracy: 0.766667\n",
            "Submission 5:\n",
            " Validation accuracy: 0.722163\n",
            " Test accuracy: 0.744444\n",
            "Submission 6:\n",
            " Validation accuracy: 0.727466\n",
            " Test accuracy: 0.766667\n",
            "Submission 7:\n",
            " Validation accuracy: 0.737010\n",
            " Test accuracy: 0.755556\n",
            "Submission 8:\n",
            " Validation accuracy: 0.760339\n",
            " Test accuracy: 0.788889\n",
            "Submission 9:\n",
            " Validation accuracy: 0.727466\n",
            " Test accuracy: 0.777778\n",
            "Submission 10:\n",
            " Validation accuracy: 0.645811\n",
            " Test accuracy: 0.644444\n",
            "Submission 11:\n",
            " Validation accuracy: 0.679745\n",
            " Test accuracy: 0.600000\n",
            "Submission 12:\n",
            " Validation accuracy: 0.734889\n",
            " Test accuracy: 0.766667\n",
            "Submission 13:\n",
            " Validation accuracy: 0.621421\n",
            " Test accuracy: 0.633333\n",
            "Submission 14:\n",
            " Validation accuracy: 0.713680\n",
            " Test accuracy: 0.744444\n",
            "Submission 15:\n",
            " Validation accuracy: 0.721103\n",
            " Test accuracy: 0.744444\n",
            "Submission 16:\n",
            " Validation accuracy: 0.694592\n",
            " Test accuracy: 0.733333\n",
            "Submission 17:\n",
            " Validation accuracy: 0.724284\n",
            " Test accuracy: 0.755556\n",
            "Submission 18:\n",
            " Validation accuracy: 0.768823\n",
            " Test accuracy: 0.733333\n",
            "Submission 19:\n",
            " Validation accuracy: 0.728526\n",
            " Test accuracy: 0.755556\n",
            "Submission 20:\n",
            " Validation accuracy: 0.648993\n",
            " Test accuracy: 0.611111\n",
            "Submission 21:\n",
            " Validation accuracy: 0.714740\n",
            " Test accuracy: 0.755556\n",
            "Submission 22:\n",
            " Validation accuracy: 0.769883\n",
            " Test accuracy: 0.766667\n",
            "Submission 23:\n",
            " Validation accuracy: 0.720042\n",
            " Test accuracy: 0.733333\n",
            "Submission 24:\n",
            " Validation accuracy: 0.730647\n",
            " Test accuracy: 0.777778\n",
            "Submission 25:\n",
            " Validation accuracy: 0.753977\n",
            " Test accuracy: 0.788889\n",
            "Submission 26:\n",
            " Validation accuracy: 0.645811\n",
            " Test accuracy: 0.644444\n",
            "Submission 27:\n",
            " Validation accuracy: 0.748674\n",
            " Test accuracy: 0.755556\n",
            "Submission 28:\n",
            " Validation accuracy: 0.686108\n",
            " Test accuracy: 0.733333\n",
            "Submission 29:\n",
            " Validation accuracy: 0.688229\n",
            " Test accuracy: 0.600000\n",
            "Submission 30:\n",
            " Validation accuracy: 0.747614\n",
            " Test accuracy: 0.777778\n",
            "Submission 31:\n",
            " Validation accuracy: 0.747614\n",
            " Test accuracy: 0.755556\n",
            "Submission 32:\n",
            " Validation accuracy: 0.741251\n",
            " Test accuracy: 0.755556\n",
            "Submission 33:\n",
            " Validation accuracy: 0.728526\n",
            " Test accuracy: 0.755556\n",
            "Submission 34:\n",
            " Validation accuracy: 0.713680\n",
            " Test accuracy: 0.744444\n",
            "Submission 35:\n",
            " Validation accuracy: 0.721103\n",
            " Test accuracy: 0.744444\n",
            "Submission 36:\n",
            " Validation accuracy: 0.728526\n",
            " Test accuracy: 0.755556\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df_train = pd.read_csv('tweet_predictions_cv.csv', header=None)\n",
        "df_test = pd.read_csv('tweet_predictions_test.csv', header=None)\n",
        "\n",
        "Y_train = df_train.iloc[0:, 0].values\n",
        "X_train = df_train.iloc[0:, 1:].values\n",
        "\n",
        "Y_test = df_test.iloc[0:, 0].values\n",
        "X_test = df_test.iloc[0:, 1:].values\n",
        "\n",
        "for i in np.arange(0, len(X_train[0]), 3):\n",
        "    print(\"Submission %d:\" % (1 + int(i/3)))\n",
        "    predictions_cv = [np.argmax(x)-1 for x in X_train[0:, i:i+3]]\n",
        "    print(\" Validation accuracy: %0.6f\" % accuracy_score(Y_train, predictions_cv))\n",
        "    predictions_test = [np.argmax(x)-1 for x in X_test[0:, i:i+3]]\n",
        "    print(\" Test accuracy: %0.6f\" % accuracy_score(Y_test, predictions_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW0KpI0cf_IJ"
      },
      "source": [
        "## Problem 1: Ensemble Classifier: Stacking\n",
        "\n",
        "First, build a classifier that uses the probabilities from the 36 submissions as features. Since each submission contains 3 probabilities, there are 108 total features.\n",
        "\n",
        "Following HW4, you should use multinomial logistic regression as the classifier. Use `sklearn`'s [`LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class, setting the `multi_class` argument to `'multinomial'`, the `solver` argument to `'lbfgs'`, and the `random_state` argument to `123` (as usual).\n",
        "\n",
        "Additionally, use [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to select the `C` parameter using 5-fold cross-validation. For the grid search, try the following values for `C`: ${0.1, 0.2, 0.3, 0.4, \\ldots, 1.8, 1.9, 2.0}$. (You can easily generate this list of values using [`numpy.arange`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.arange.html).) When making predictions on the test data, you should use the optimal classifier tuned during cross-validation.\n",
        "\n",
        "You may wish to refer to the HW4 code to get started, since the code will be similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSlmbCruf_IK"
      },
      "source": [
        "#### Deliverable 1.1: Implement the ensemble classifier as described, and calculate both the cross-validation accuracy and test accuracy.\n",
        "\n",
        "[output below]\n",
        "\n",
        "#### Deliverable 1.2: Examine the validation and test accuracies of the individual submissions above. How do these accuracies compare to the validation and test accuracy of your ensemble classifier?\n",
        "\n",
        "\n",
        "#### Deliverable 1.3: Based on what was discussed in lecture, explain these results. If the ensemble outperformed the individual classifiers, explain why ensembles are able to do this. If the ensemble did not outperform the individual classifiers, explain why this particular ensemble might not have been effective.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "CNpJEABTgqoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HahqLbiAf_IL",
        "outputId": "b11c5711-c59a-416e-b5e5-8b94bb491791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy: 0.814415\n",
            "Test accuracy: 0.800000\n"
          ]
        }
      ],
      "source": [
        "#1.1\n",
        "params = [{'C' : np.arange(0.1,2.1,0.1)}]\n",
        "lgr = LogisticRegression(multi_class='multinomial',solver='lbfgs',max_iter = 500,random_state=123)\n",
        "\n",
        "gs_classifier = GridSearchCV(lgr,params,cv=5)\n",
        "gs_classifier.fit(X_train,Y_train)\n",
        "\n",
        "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)\n",
        "print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, gs_classifier.predict(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.2\n",
        "'''\n",
        "The ensemble classifier greatly outpreforms the individual classifiers in both validation and test accuracy.\n",
        "The highest score achieved for both of the individual classifiers was 72% and 75% respectively vs.\n",
        "a score of 81% and 80% for the ensemble classifier\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "tEpYnrdkkP_z",
        "outputId": "852fb2ab-fc13-4046-eed3-93d7681dd6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe ensemble classifier greatly outpreforms the individual classifiers in both validation and test accuracy.\\nThe highest score achieved for both of the individual classifiers was 72% and 75% respectively vs.\\na score of 81% and 80% for the ensemble classifier\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.3\n",
        "'''\n",
        "Ensembles may outpreform individual classifiers for a multitude of reasons. One reason the ensemble outpreformed\n",
        "the individual classifiers is due to the diversity of the individual classifiers. Each classifier had a range of\n",
        "accuracy scores (both validation and test) some ranging ~68% to ~79%. When there is high diversity in an ensemble\n",
        "classifier it is able to utilize the strengths or reduce weakness of the resultant classifier or prevent\n",
        "overfitting on the dataset\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Wa0tG7LolMPh",
        "outputId": "dde8f285-5984-4f89-c183-dc87ed5cef7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y16zIUEBf_IL"
      },
      "source": [
        "## Problem 2: Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIXmdZtOf_IM"
      },
      "source": [
        "Since the features are continuous-valued and correlated with each other, this feature set is a good candidate for dimensionality reduction with principal component analysis (PCA). You will experiment with PCA here.\n",
        "\n",
        "Use the [`sklearn.decomposition.PCA`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) class to transform the feature vectors (`X_train` and `X_test`) using PCA.  You should fit PCA with the training data, and then transform the feature vectors of both the training and test data. This will require a combination of the `fit`, `transform`, and/or `fit_transform` functions.\n",
        "\n",
        "When creating a `PCA` object, you set the number of components (that is, the dimensionality of the feature vectors) with the `n_components` argument. Additionally, set `random_state` to `123`.\n",
        "\n",
        "You should run the same classifier from Problem 1 on the PCA-reduced data. You should continue to use `GridSearchCV` to tune `C`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIE70sdJf_IM"
      },
      "source": [
        "#### Deliverable 2.1: Apply PCA to the data and calculate the validation and test accuracies when the number of components is each of: $1, 2, 10, 20, 30, 40, 50, 100$.\n",
        "\n",
        "[you may wish to plot these results, but it is not required as long as your results are readable]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "cJfLLNsmf_IM"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "components = [1,2,10,20,30,40,50,100]\n",
        "for p in components:\n",
        "  pipe_lr = make_pipeline(PCA(n_components=p,random_state=123),lgr)\n",
        "  pipe_lr.fit(X_train,Y_train)\n",
        "\n",
        "  print(f'components = {p}')\n",
        "  print(\"Validation accuracy: %0.6f\" % accuracy_score(Y_train, pipe_lr.predict(X_train)))\n",
        "  print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, pipe_lr.predict(X_test)))\n",
        "  print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzToGTBBqi8N",
        "outputId": "d2726c97-d270-4228-f619-5f9fc73ce4ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "components = 1\n",
            "Validation accuracy: 0.663839\n",
            "Test accuracy: 0.677778\n",
            "\n",
            "components = 2\n",
            "Validation accuracy: 0.762460\n",
            "Test accuracy: 0.777778\n",
            "\n",
            "components = 10\n",
            "Validation accuracy: 0.786850\n",
            "Test accuracy: 0.811111\n",
            "\n",
            "components = 20\n",
            "Validation accuracy: 0.816543\n",
            "Test accuracy: 0.777778\n",
            "\n",
            "components = 30\n",
            "Validation accuracy: 0.834571\n",
            "Test accuracy: 0.811111\n",
            "\n",
            "components = 40\n",
            "Validation accuracy: 0.848356\n",
            "Test accuracy: 0.800000\n",
            "\n",
            "components = 50\n",
            "Validation accuracy: 0.847296\n",
            "Test accuracy: 0.777778\n",
            "\n",
            "components = 100\n",
            "Validation accuracy: 0.854719\n",
            "Test accuracy: 0.800000\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}